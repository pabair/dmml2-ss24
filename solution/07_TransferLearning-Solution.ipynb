{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe633a53",
   "metadata": {},
   "source": [
    "# Transfer Learning with Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f04e7",
   "metadata": {},
   "source": [
    "In this notebook we load a small datasets that contains dolphins and elephants. We classify the images using CNNs and compare two approaches and see what worsk better:\n",
    "1. Training a CNN from scratch against\n",
    "2. Finetuning a pretrained ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9347627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcab066d450>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ba09d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ce7b5",
   "metadata": {},
   "source": [
    "Let's load our data a have a look at the shape of some images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0508c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=RGB size=300x179 at 0x7FCAAF5CBDF0>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x179 at 0x7FCAAF5CBE50>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x166 at 0x7FCAAF5CBF40>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x259 at 0x7FCAAF5CB9A0>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x225 at 0x7FCAAF5CBDF0>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x277 at 0x7FCAAF5CBFA0>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x183 at 0x7FCAAF5CBF40>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x225 at 0x7FCAAF5CBE50>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x214 at 0x7FCAAF5CBDF0>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x223 at 0x7FCAAF5CB9A0>, 0)\n",
      "(<PIL.Image.Image image mode=RGB size=300x277 at 0x7FCAAF5CBF40>, 0)\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.ImageFolder(root='../data/animals')\n",
    "for i, data in enumerate(dataset):\n",
    "    print(data)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d4e264",
   "metadata": {},
   "source": [
    "We see that the pictures have all width=300 but a varying height. To use them in transfer learning they need to have the standard shape (224, 224), which is the data format of ImageNet (on which most pretrained models are trained on).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb2c83",
   "metadata": {},
   "source": [
    "To get them into this shape, we first increase the height to 224 (this will also increase the height) and then take the 224 square which is center in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551777c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "             transforms.Resize(size=224),\n",
    "             transforms.CenterCrop(size=224),\n",
    "             transforms.ToTensor(),\n",
    "             transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225]) # standard normalization for transfer learning\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f869668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 data points\n"
     ]
    }
   ],
   "source": [
    "data = datasets.ImageFolder(root='../data/animals', transform=image_transforms)\n",
    "\n",
    "print(len(data), \"data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4e3626",
   "metadata": {},
   "source": [
    "Next, we split the data into train and test and define the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43d1ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = torch.utils.data.random_split(data, [100, 29])\n",
    "\n",
    "batch_size = 10\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=29,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778bce70",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "### Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8fafcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 4, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(4*111*111, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 4*111*111)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "314faedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, loss: 327.91980\n",
      "Epoch:2, loss: 18.60620\n",
      "Epoch:3, loss: 6.36712\n",
      "Epoch:4, loss: 4.56148\n",
      "Epoch:5, loss: 4.13382\n",
      "Epoch:6, loss: 3.93716\n",
      "Epoch:7, loss: 3.84604\n",
      "Epoch:8, loss: 3.66799\n",
      "Epoch:9, loss: 3.49616\n",
      "Epoch:10, loss: 3.59663\n",
      "Epoch:11, loss: 3.19508\n",
      "Epoch:12, loss: 3.03328\n",
      "Epoch:13, loss: 2.87331\n",
      "Epoch:14, loss: 2.71855\n",
      "Epoch:15, loss: 2.60966\n",
      "Epoch:16, loss: 2.42548\n",
      "Epoch:17, loss: 2.27410\n",
      "Epoch:18, loss: 2.13496\n",
      "Epoch:19, loss: 1.99203\n",
      "Epoch:20, loss: 1.87148\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    running_loss = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch:{epoch + 1}, loss: {running_loss:.5f}')\n",
    "    running_loss = 0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d7edaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 82 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "wrong_images = []\n",
    "wrong_labesl = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        predicted = torch.argmax(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd04a89",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "Instead of training a CNN from scratch, load a pretrained ResNet18 and only train the last layer. Train again for 20 epochs and compare the results.\n",
    "Use this PyTorch tutorial to see how this works (section `ConvNet as fixed feature extractor`):\n",
    "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7db263f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "my_layer = nn.Linear(model.fc.in_features, 2) # new last layer\n",
    "model.fc = my_layer # replace fc layer from ResNet with my layer\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c61aa963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, loss: 6.86336\n",
      "2, loss: 2.45239\n",
      "3, loss: 2.25217\n",
      "4, loss: 2.71460\n",
      "5, loss: 1.18777\n",
      "6, loss: 1.98528\n",
      "7, loss: 1.68125\n",
      "8, loss: 0.67823\n",
      "9, loss: 2.76517\n",
      "10, loss: 2.69081\n",
      "11, loss: 0.58167\n",
      "12, loss: 0.46639\n",
      "13, loss: 0.85015\n",
      "14, loss: 0.52710\n",
      "15, loss: 0.63971\n",
      "16, loss: 0.65163\n",
      "17, loss: 0.31829\n",
      "18, loss: 0.55537\n",
      "19, loss: 0.39761\n",
      "20, loss: 0.75264\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net = model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "\n",
    "    loss_batch = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        #print(outputs.shape)\n",
    "        #print(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_batch += loss.item()\n",
    "        \n",
    "    print(f'{epoch + 1}, loss: {loss_batch:.5f}')\n",
    "    loss_batch = 0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e3c5f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "wrong_images = []\n",
    "wrong_labesl = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        predicted = torch.argmax(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
