{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Neural Networks with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will build our first neural network using only `numpy` as library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work on the same dataset as last week and try to predict which digit is shown on the given pixel values. This time we load the dataset directly from sklearn using 28x28 pixels to show the digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, data_home=\"./data\", cache=True, parser='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know already from last time how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the following histogram we see that the pixel values are between `0` and `255`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[0].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label is a number between 0-9 representing the digit shown on the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, the label is given as a digit. However, to calculate the loss function of the neural network, we need the label as a one-hot-encoded version, in which the true digit is encoded as `1` and the rest as `0`.\n",
    "\n",
    "For instance:\n",
    "- `3` -> `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`\n",
    "- `9` -> `[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]`\n",
    "\n",
    "This is done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y_categorical = pd.get_dummies(y).astype('float32').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first five lines to check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_categorical[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we scale the data in the range [0,1] and divide it into train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_scaled = (X/255).astype('float32').values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_categorical, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Implement the Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the following structure of a neural network with two hidden layers:\n",
    "- Input layer of size 784 with sigmoid activation\n",
    "- First hidden layer of size 128 with sigmoid activation\n",
    "- Second hidden layer of size 64 with sigmoid activation\n",
    "- Output layer of size 10 with softmax activation\n",
    "\n",
    "A skeleton code for this network is given in the following class. Your first task is to complete the method `forward_pass` to calculate the forward pass on __one__ data point. Below this cell you can find a test to check whether your implementation is correct. Also see the hint below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class DeepNeuralNetwork():\n",
    "    \n",
    "    # do not touch this method\n",
    "    def __init__(self):\n",
    "        \n",
    "        # initialize weights randomly\n",
    "        np.random.seed(0)\n",
    "        self.w1 = np.random.randn(128, 784)\n",
    "        self.w2 = np.random.randn(64, 128)\n",
    "        self.w3 = np.random.randn(10, 64)\n",
    "\n",
    "    def forward_pass(self, x_train):\n",
    "        \n",
    "        z1 = # implement the dot product for w1 * x_train\n",
    "        a1 = # implement the sigmoid activation on z1\n",
    "        z2 = # implement the dot product for w2 * a1\n",
    "        a2 = # implement the sigmoid activation on z2\n",
    "        z3 = # implement the dot product for w3 * a2\n",
    "        a3 = # implement the softmax activation for z3\n",
    "        \n",
    "        # we need to remember all values for the gradient calculation, don't touch this\n",
    "        self.fwdpass = [x_train, z1, a1, z2, a2, z3, a3]\n",
    "        \n",
    "        return a3\n",
    "    \n",
    "    # do not touch this method\n",
    "    def get_gradients(self, y, y_hat):\n",
    "        # restore values from foward pass\n",
    "        a0, z1, a1, z2, a2, z3, a3 = self.fwdpass\n",
    "        \n",
    "        # Calculate W3 update\n",
    "        exps = np.exp(z3 - z3.max())\n",
    "        softmax_derivative = exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "        error = 2 * (y_hat - y) / y_hat.shape[0] * softmax_derivative\n",
    "        gradient_w3 = np.outer(error, a2)\n",
    "\n",
    "        # Calculate W2 update\n",
    "        sigmoid_derivative = (np.exp(-z2))/((np.exp(-z2)+1)**2)\n",
    "        error = np.dot(self.w3.T, error) * sigmoid_derivative\n",
    "        gradient_w2 = np.outer(error, a1)\n",
    "\n",
    "        # Calculate W1 update\n",
    "        sigmoid_derivative = (np.exp(-z1))/((np.exp(-z1)+1)**2)\n",
    "        error = np.dot(self.w2.T, error) * sigmoid_derivative\n",
    "        gradient_w1 = np.outer(error, a0)\n",
    "\n",
    "        return [gradient_w1, gradient_w2, gradient_w3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: To calculate the ouput of a layer you can use numpys matrix operations, as we have seen it in the lecture slides.\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "w = np.array([[2,2,2],[1,1,1]])\n",
    "print(w)\n",
    "print(w.shape)\n",
    "x = np.array([3, 4, 5])\n",
    "print(x.shape)\n",
    "\n",
    "z = np.dot(w, x)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for task 1:\n",
    "dnn = DeepNeuralNetwork()\n",
    "\n",
    "# the network outputs a probability for every neuron in the last layer\n",
    "y_hat = dnn.forward_pass(X_train[0])\n",
    "print(\"The output of the last layer looks like this:\\n\", y_hat)\n",
    "\n",
    "# to check if the network works correctly, check if the following condition is True\n",
    "abs(y_hat[8] - 0.946) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Implement  the training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start training the network by implementing the training procedure. We train the network for 10 epochs as shown in the code below.\n",
    "In each epoch we go over every data point `x` in `X_train` and:\n",
    "1. Calculate a forward pass on `x` and save it as `y_hat`. Use the `forward_pass` method that you implemeneted in the previous task.\n",
    "2. Calculate the gradients for the weight in w1, w2 and w3 using the `get_gradient` function of the network\n",
    "3. Update the weights `w1`, `w2` and `w3` of the network by moving into the negative direction of the gradient multiplied with the `learning_rate`\n",
    "4. Bonus: Calculate the cross-entropy-loss after each epoch and plot it in relation to the epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = DeepNeuralNetwork()\n",
    "\n",
    "no_epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "start_time = time.time()\n",
    "losses = []\n",
    "for iteration in range(no_epochs):\n",
    "    # your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Predict on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the network is trained, we can use it to predict on the test data.\n",
    "\n",
    "Task: \n",
    "- Iterate over the test data and use the trained network the predict on every test data point.\n",
    "- Identify the index of the neuron which returned the highest probability.\n",
    "- Compare this value to the true label in the test data.\n",
    "- Compute the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Task:\n",
    "- Remove the first hidden layer. Train the network and check the performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
