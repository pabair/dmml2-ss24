{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for Classifying Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are building and training a basic character-level RNN to classify\n",
    "words. A character-level RNN reads words as a series of characters -\n",
    "outputting a prediction and \"hidden state\" at each step, feeding its\n",
    "previous hidden state into each next time step. We take the final prediction\n",
    "to be the output, i.e. which class the word belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "\n",
    "Download the data in folder `data/names` from GitHub.\n",
    "\n",
    "Included in the ``data/names`` directory are 18 text files named as\n",
    "``[Language].txt``. Each file contains a bunch of names, one name per\n",
    "line, mostly romanized (but we still need to convert from Unicode to\n",
    "ASCII)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get all the filenames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Czech.txt', 'data/names/German.txt', 'data/names/Arabic.txt', 'data/names/Japanese.txt', 'data/names/Chinese.txt', 'data/names/Vietnamese.txt', 'data/names/Russian.txt', 'data/names/French.txt', 'data/names/Irish.txt', 'data/names/English.txt', 'data/names/Spanish.txt', 'data/names/Greek.txt', 'data/names/Italian.txt', 'data/names/Portuguese.txt', 'data/names/Scottish.txt', 'data/names/Dutch.txt', 'data/names/Korean.txt', 'data/names/Polish.txt']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "filenames = glob.glob('data/names/*.txt')\n",
    "\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save each language as a category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese', 'Russian', 'French', 'Irish', 'English', 'Spanish', 'Greek', 'Italian', 'Portuguese', 'Scottish', 'Dutch', 'Korean', 'Polish']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "all_categories = []\n",
    "\n",
    "\n",
    "for filename in filenames:\n",
    "    language = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(language)\n",
    "\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the data and put every name in a list together and its category (=label) in a second list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 20074)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "\n",
    "for index, filename in enumerate(filenames):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    category = all_categories[index]\n",
    "    for line in lines:\n",
    "        X.append(line)\n",
    "        y.append(category)\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "n_categories, len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E', 'p', 'ú', 'ä', 'Ś', 'g', '/', 'ò', 'Q', ':', \"'\", 'n', 'i', ',', 'D', 'M', 'l', 'K', 'k', 'á', 'z', 'J', 'c', 'v', 'G', 'Z', 'u', 'õ', 'A', 'à', 'b', 'N', 'T', 'y', 'B', 'í', 'x', 'm', 'P', 'U', 'è', 'ż', 'Ż', 'w', 'L', 'O', 'S', 'Y', 't', '\\xa0', 'ö', 'C', 's', 'ß', 'ń', 'W', 'q', 'ą', 'e', 'ü', 'ñ', 'a', 'F', 'ù', 'h', 'ã', 'Á', 'ê', 'ç', 'V', 'j', 'ì', 'r', 'X', 'É', 'ó', 'f', 'é', ' ', 'R', 'o', '1', 'ł', '-', 'H', 'd', 'I'}\n",
      "87 characters\n"
     ]
    }
   ],
   "source": [
    "all_characters = set([c for name in X for c in name])\n",
    "all_characters = set([c for name in X for c in name])\n",
    "print(all_characters)\n",
    "print(len(all_characters), \"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the files contain many special characters that make our problem more difficult. To reduce the character count, we only allow ASCII symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab is of size 52 and contains: abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# these is the vocabulary we will use\n",
    "all_letters = string.ascii_letters\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "print(f\"Vocab is of size {n_letters} and contains:\", all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n",
      "Fruhling\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "# this method converts anything into ascii\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))\n",
    "print(unicodeToAscii('Frühling'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'j', 'E', 'p', 'r', 'z', 'C', 'c', 'v', 'J', 'X', 's', 'g', 'G', 'Z', 'W', 'Q', 'f', 'q', 'u', 'e', 'A', 'n', 'R', 'i', 'o', 'b', 'N', 'T', 'D', 'y', 'a', 'B', 'F', 'M', 'x', 'm', 'h', 'P', 'U', 'l', 'K', 'w', 'k', 'L', 'O', 'H', 'd', 'S', 'Y', 'I', 't', 'V'}\n",
      "52 characters\n"
     ]
    }
   ],
   "source": [
    "# convert all letters to ascii\n",
    "X = [unicodeToAscii(x) for x in X]\n",
    "\n",
    "# print again all characters\n",
    "all_characters = set([c for name in X for c in name])\n",
    "print(all_characters)\n",
    "print(len(all_characters), \"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we successfully reduced the number of characters and can now divide the data into train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data points: 16059\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Train data points:\", len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning Names into Tensors\n",
    "--------------------------\n",
    "\n",
    "Now that we have all the names organized, we need to turn them into\n",
    "Tensors to make any use of them.\n",
    "\n",
    "To represent a single letter, we use a \"one-hot vector\" of size\n",
    "``<1 x n_letters>``. A one-hot vector is filled with 0s except for a 1\n",
    "at index of the current letter, e.g. ``\"b\" = <0 1 0 0 0 ...>``.\n",
    "\n",
    "To make a word we join a bunch of those into a 2D matrix\n",
    "``<line_length x 1 x n_letters>``.\n",
    "\n",
    "That extra 1 dimension is because PyTorch assumes everything is in\n",
    "batches - we're just using a batch size of 1 here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    index = all_letters.find(letter)\n",
    "    tensor[0][index] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categoryToTensor(category):\n",
    "    index = all_categories.index(category)\n",
    "    return torch.tensor([index], dtype=torch.long)\n",
    "\n",
    "categoryToTensor(\"Korean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Network\n",
    "====================\n",
    "\n",
    "Before autograd, creating a recurrent neural network in Torch involved\n",
    "cloning the parameters of a layer over several timesteps. The layers\n",
    "held hidden state and gradients which are now entirely handled by the\n",
    "graph itself. This means you can implement a RNN in a very \"pure\" way,\n",
    "as regular feed-forward layers.\n",
    "\n",
    "This RNN module is just 2 linear layers which operate on an input and hidden state, with\n",
    "a LogSoftmax layer after the output.\n",
    "You can see the architecture here: https://i.imgur.com/Z2xbySO.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = 128 # number of hidden layer size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + self.hidden_size, self.hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + self.hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        combined = torch.cat((x, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a step of this network we need to pass an input (in our case, the\n",
    "Tensor for the current letter) and a previous hidden state (which we\n",
    "initialize as zeros at first). We'll get back the output (probability of\n",
    "each language) and a next hidden state (which we keep for the next\n",
    "step).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0498, 0.0554, 0.0567, 0.0555, 0.0581, 0.0578, 0.0486, 0.0582, 0.0582,\n",
      "         0.0569, 0.0520, 0.0560, 0.0596, 0.0572, 0.0556, 0.0567, 0.0511, 0.0568]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_letters, n_categories)\n",
    "\n",
    "x = letterToTensor('A')\n",
    "hidden = rnn.initHidden()\n",
    "\n",
    "output, next_hidden = rnn(x, hidden)\n",
    "print(torch.softmax(output, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output is a ``<1 x n_categories>`` Tensor, where\n",
    "every item is the likelihood of that category (higher is more likely).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Training the Network\n",
    "--------------------\n",
    "\n",
    "Finish the following training function to train the RNN on the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "Training epoch: 2\n",
      "Training epoch: 3\n",
      "Training epoch: 4\n",
      "Training epoch: 5\n",
      "Training epoch: 6\n",
      "Training epoch: 7\n",
      "Training epoch: 8\n",
      "Training epoch: 9\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    print(\"Training epoch:\", epoch)\n",
    "    # iterate through all names in X_train\n",
    "    # for every name:\n",
    "        # init the hidden layer of the rnn\n",
    "        # insert the name character by character into the rnn and compute the final output\n",
    "        # note: you need to carry on the hidden state in every time step\n",
    "        # define the loss on the last output of the rnn and the category (=label)\n",
    "        # backpropagate the loss and take an optimizer step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Evaluating the Results\n",
    "\n",
    "Evaluate the accuarcy of the RNN on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Running on User Input\n",
    "\n",
    "Write a function that takes an abritrary name as input and outputs the top 3 categories of the RNN for the input.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
